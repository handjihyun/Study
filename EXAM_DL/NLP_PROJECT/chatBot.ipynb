{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>침구 추가를 해야 할 것 같아서요</td>\n",
       "      <td>1세트가 필요하신 건가요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>맞습니다 추가 요금은 얼마인가요</td>\n",
       "      <td>1만 원이시고요 투숙기간 과는 무관합니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>침구문의는 어디다 말씀드려야 할까요</td>\n",
       "      <td>프론트에다가 사전에 말씀 부탁 드릴게요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>예약하려면 어떻게 해야 하나요</td>\n",
       "      <td>회원 가입 후 로그인 하셔서 예약하시면 되세요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>객실엔 세면 도구가 있나요</td>\n",
       "      <td>타월과 욕실 용품은 있지만 개인 용품은 챙겨오셔야 합니다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Q                                 A\n",
       "0   침구 추가를 해야 할 것 같아서요                     1세트가 필요하신 건가요 \n",
       "1    맞습니다 추가 요금은 얼마인가요            1만 원이시고요 투숙기간 과는 무관합니다 \n",
       "2  침구문의는 어디다 말씀드려야 할까요             프론트에다가 사전에 말씀 부탁 드릴게요 \n",
       "3     예약하려면 어떻게 해야 하나요         회원 가입 후 로그인 하셔서 예약하시면 되세요 \n",
       "4       객실엔 세면 도구가 있나요   타월과 욕실 용품은 있지만 개인 용품은 챙겨오셔야 합니다 "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('chat_data.csv', usecols=[0,1])\n",
    "df = df[:-2]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q    0\n",
       "A    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결측치 확인\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFGPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.10.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2', bos_token='', eos_token='', pad_token='')\n",
    "model = TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51200\n",
      "51200\n",
      "51200\n",
      "----------\n",
      "</s>\n",
      "<usr>\n",
      "<pad>\n",
      "<sys>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.bos_token_id)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print('-' * 10)\n",
    "print(tokenizer.decode(1))\n",
    "print(tokenizer.decode(2))\n",
    "print(tokenizer.decode(3))\n",
    "print(tokenizer.decode(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm     # 반복문 진행률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34336"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_data():\n",
    "    for question, answer in zip(df.Q.to_list(), df.A.to_list()):\n",
    "        # start of sentence\n",
    "        bos_token = [tokenizer.bos_token_id]\n",
    "\n",
    "        # end of sentence\n",
    "        eos_token = [tokenizer.eos_token_id]\n",
    "\n",
    "        sent = tokenizer.encode('' + question + '' + answer)\n",
    "\n",
    "        # 결과값을 나누어 얻음\n",
    "        yield bos_token + sent + eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(get_chat_data, output_types = tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.padded_batch(batch_size = batch_size, padded_shapes=(None,),\n",
    "padding_values=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 36)\n",
      "(32, 31)\n",
      "(32, 26)\n",
      "(32, 49)\n",
      "(32, 50)\n",
      "(32, 51)\n",
      "(32, 44)\n",
      "(32, 55)\n",
      "(32, 57)\n",
      "(32, 62)\n",
      "(32, 50)\n",
      "(32, 59)\n",
      "(32, 51)\n",
      "(32, 53)\n",
      "(32, 49)\n",
      "(32, 58)\n",
      "(32, 54)\n",
      "(32, 45)\n",
      "(32, 49)\n",
      "(32, 45)\n",
      "(32, 50)\n",
      "(32, 52)\n",
      "(32, 64)\n",
      "(32, 52)\n",
      "(32, 52)\n",
      "(32, 49)\n",
      "(32, 42)\n",
      "(32, 44)\n",
      "(32, 50)\n",
      "(32, 41)\n",
      "(32, 40)\n",
      "(32, 45)\n",
      "(32, 40)\n",
      "(32, 37)\n",
      "(32, 45)\n",
      "(32, 52)\n",
      "(32, 55)\n",
      "(32, 46)\n",
      "(32, 41)\n",
      "(32, 57)\n",
      "(32, 39)\n",
      "(32, 41)\n",
      "(32, 39)\n",
      "(32, 50)\n",
      "(32, 52)\n",
      "(32, 46)\n",
      "(32, 50)\n",
      "(32, 43)\n",
      "(32, 53)\n",
      "(32, 48)\n",
      "(32, 51)\n",
      "(32, 47)\n",
      "(32, 57)\n",
      "(32, 43)\n",
      "(32, 51)\n",
      "(32, 48)\n",
      "(32, 52)\n",
      "(32, 47)\n",
      "(32, 48)\n",
      "(32, 53)\n",
      "(32, 47)\n",
      "(32, 41)\n",
      "(32, 59)\n",
      "(32, 54)\n",
      "(32, 58)\n",
      "(32, 50)\n",
      "(32, 48)\n",
      "(32, 74)\n",
      "(32, 65)\n",
      "(32, 35)\n",
      "(32, 56)\n",
      "(32, 56)\n",
      "(32, 44)\n",
      "(32, 48)\n",
      "(32, 50)\n",
      "(32, 56)\n",
      "(32, 41)\n",
      "(32, 40)\n",
      "(32, 35)\n",
      "(32, 39)\n",
      "(32, 48)\n",
      "(32, 50)\n",
      "(32, 53)\n",
      "(32, 48)\n",
      "(32, 52)\n",
      "(32, 55)\n",
      "(32, 46)\n",
      "(32, 47)\n",
      "(32, 51)\n",
      "(32, 47)\n",
      "(32, 52)\n",
      "(32, 41)\n",
      "(32, 42)\n",
      "(32, 39)\n",
      "(32, 35)\n",
      "(32, 26)\n",
      "(32, 25)\n",
      "(32, 30)\n",
      "(32, 33)\n",
      "(32, 32)\n",
      "(32, 46)\n",
      "(32, 54)\n",
      "(32, 46)\n",
      "(32, 55)\n",
      "(32, 50)\n",
      "(32, 51)\n",
      "(32, 60)\n",
      "(32, 54)\n",
      "(32, 58)\n",
      "(32, 54)\n",
      "(32, 38)\n",
      "(32, 37)\n",
      "(32, 40)\n",
      "(32, 40)\n",
      "(32, 33)\n",
      "(32, 38)\n",
      "(32, 40)\n",
      "(32, 36)\n",
      "(32, 34)\n",
      "(32, 40)\n",
      "(32, 49)\n",
      "(32, 52)\n",
      "(32, 50)\n",
      "(32, 43)\n",
      "(32, 43)\n",
      "(32, 53)\n",
      "(32, 67)\n",
      "(32, 74)\n",
      "(32, 68)\n",
      "(32, 72)\n",
      "(32, 51)\n",
      "(32, 42)\n",
      "(32, 58)\n",
      "(32, 40)\n",
      "(32, 41)\n",
      "(32, 41)\n",
      "(32, 56)\n",
      "(32, 53)\n",
      "(32, 52)\n",
      "(32, 37)\n",
      "(32, 50)\n",
      "(32, 45)\n",
      "(32, 51)\n",
      "(32, 49)\n",
      "(32, 39)\n",
      "(32, 52)\n",
      "(32, 50)\n",
      "(32, 37)\n",
      "(32, 41)\n",
      "(32, 41)\n",
      "(32, 47)\n",
      "(32, 43)\n",
      "(32, 45)\n",
      "(32, 43)\n",
      "(32, 39)\n",
      "(32, 40)\n",
      "(32, 38)\n",
      "(32, 36)\n",
      "(32, 32)\n",
      "(32, 55)\n",
      "(32, 62)\n",
      "(32, 43)\n",
      "(32, 44)\n",
      "(32, 33)\n",
      "(32, 42)\n",
      "(32, 39)\n",
      "(32, 34)\n",
      "(32, 33)\n",
      "(32, 43)\n",
      "(32, 42)\n",
      "(32, 50)\n",
      "(32, 50)\n",
      "(32, 55)\n",
      "(32, 37)\n",
      "(32, 50)\n",
      "(32, 43)\n",
      "(32, 59)\n",
      "(32, 67)\n",
      "(32, 65)\n",
      "(32, 73)\n",
      "(32, 54)\n",
      "(32, 53)\n",
      "(32, 46)\n",
      "(32, 53)\n",
      "(32, 53)\n",
      "(32, 48)\n",
      "(32, 49)\n",
      "(32, 44)\n",
      "(32, 64)\n",
      "(32, 55)\n",
      "(32, 59)\n",
      "(32, 54)\n",
      "(32, 48)\n",
      "(32, 41)\n",
      "(32, 41)\n",
      "(32, 42)\n",
      "(32, 44)\n",
      "(32, 41)\n",
      "(32, 48)\n",
      "(32, 44)\n",
      "(32, 52)\n",
      "(32, 50)\n",
      "(32, 50)\n",
      "(32, 49)\n",
      "(32, 47)\n",
      "(32, 81)\n",
      "(32, 68)\n",
      "(32, 93)\n",
      "(32, 55)\n",
      "(32, 66)\n",
      "(32, 66)\n",
      "(32, 48)\n",
      "(32, 52)\n",
      "(32, 55)\n",
      "(32, 64)\n",
      "(32, 40)\n",
      "(32, 43)\n",
      "(32, 50)\n",
      "(32, 59)\n",
      "(32, 54)\n",
      "(32, 39)\n",
      "(32, 44)\n",
      "(32, 31)\n",
      "(32, 44)\n",
      "(32, 34)\n",
      "(32, 33)\n",
      "(32, 42)\n",
      "(32, 35)\n",
      "(32, 36)\n",
      "(32, 48)\n",
      "(32, 46)\n",
      "(32, 47)\n",
      "(32, 48)\n",
      "(32, 48)\n",
      "(32, 46)\n",
      "(32, 65)\n",
      "(32, 48)\n",
      "(32, 50)\n",
      "(32, 57)\n",
      "(32, 54)\n",
      "(32, 47)\n",
      "(32, 54)\n",
      "(32, 52)\n",
      "(32, 44)\n",
      "(32, 60)\n",
      "(32, 60)\n",
      "(32, 58)\n",
      "(32, 57)\n",
      "(32, 49)\n",
      "(32, 47)\n",
      "(32, 48)\n",
      "(32, 47)\n",
      "(32, 32)\n",
      "(32, 31)\n",
      "(32, 35)\n",
      "(32, 49)\n",
      "(32, 41)\n",
      "(32, 44)\n",
      "(32, 46)\n",
      "(32, 47)\n",
      "(32, 35)\n",
      "(32, 32)\n",
      "(32, 61)\n",
      "(32, 51)\n",
      "(32, 53)\n",
      "(32, 58)\n",
      "(32, 48)\n",
      "(32, 51)\n",
      "(32, 53)\n",
      "(32, 54)\n",
      "(32, 53)\n",
      "(32, 49)\n",
      "(32, 49)\n",
      "(32, 53)\n",
      "(32, 52)\n",
      "(32, 59)\n",
      "(32, 49)\n",
      "(32, 50)\n",
      "(32, 77)\n",
      "(32, 64)\n",
      "(32, 59)\n",
      "(32, 75)\n",
      "(32, 48)\n",
      "(32, 47)\n",
      "(32, 50)\n",
      "(32, 50)\n",
      "(32, 52)\n",
      "(32, 57)\n",
      "(32, 37)\n",
      "(32, 43)\n",
      "(32, 47)\n",
      "(32, 51)\n",
      "(32, 35)\n",
      "(32, 30)\n",
      "(32, 31)\n",
      "(32, 34)\n",
      "(32, 41)\n",
      "(32, 47)\n",
      "(32, 35)\n",
      "(32, 38)\n",
      "(32, 41)\n",
      "(32, 37)\n",
      "(32, 38)\n",
      "(32, 34)\n",
      "(32, 38)\n",
      "(32, 40)\n",
      "(32, 64)\n",
      "(32, 44)\n",
      "(32, 43)\n",
      "(32, 44)\n",
      "(32, 47)\n",
      "(32, 43)\n",
      "(32, 39)\n",
      "(32, 41)\n",
      "(32, 51)\n",
      "(32, 38)\n",
      "(32, 36)\n",
      "(32, 34)\n",
      "(32, 39)\n",
      "(32, 28)\n",
      "(32, 37)\n",
      "(32, 34)\n",
      "(32, 36)\n",
      "(32, 35)\n",
      "(32, 40)\n",
      "(32, 48)\n",
      "(32, 41)\n",
      "(32, 41)\n",
      "(32, 46)\n",
      "(32, 48)\n",
      "(32, 56)\n",
      "(32, 41)\n",
      "(32, 39)\n",
      "(32, 56)\n",
      "(32, 47)\n",
      "(32, 53)\n",
      "(32, 52)\n",
      "(32, 55)\n",
      "(32, 48)\n",
      "(32, 47)\n",
      "(32, 42)\n",
      "(32, 38)\n",
      "(32, 35)\n",
      "(32, 31)\n",
      "(32, 39)\n",
      "(32, 37)\n",
      "(32, 46)\n",
      "(32, 45)\n",
      "(32, 35)\n",
      "(32, 40)\n",
      "(32, 49)\n",
      "(32, 61)\n",
      "(32, 43)\n",
      "(32, 50)\n",
      "(32, 61)\n",
      "(32, 41)\n",
      "(32, 38)\n",
      "(32, 48)\n",
      "(32, 50)\n",
      "(32, 92)\n",
      "(32, 63)\n",
      "(32, 59)\n",
      "(32, 48)\n",
      "(32, 60)\n",
      "(32, 63)\n",
      "(32, 63)\n",
      "(32, 54)\n",
      "(32, 65)\n",
      "(32, 53)\n",
      "(32, 55)\n",
      "(32, 49)\n",
      "(32, 50)\n",
      "(32, 49)\n",
      "(32, 46)\n",
      "(32, 53)\n",
      "(32, 43)\n",
      "(32, 48)\n",
      "(32, 39)\n",
      "(32, 50)\n",
      "(32, 56)\n",
      "(32, 47)\n",
      "(32, 55)\n",
      "(32, 62)\n",
      "(32, 44)\n",
      "(32, 62)\n",
      "(32, 53)\n",
      "(32, 53)\n",
      "(32, 71)\n",
      "(32, 60)\n",
      "(32, 73)\n",
      "(32, 81)\n",
      "(32, 51)\n",
      "(32, 55)\n",
      "(32, 55)\n",
      "(32, 57)\n",
      "(32, 57)\n",
      "(32, 60)\n",
      "(32, 58)\n",
      "(32, 41)\n",
      "(32, 52)\n",
      "(32, 66)\n",
      "(32, 49)\n",
      "(32, 47)\n",
      "(32, 43)\n",
      "(32, 48)\n",
      "(32, 37)\n",
      "(32, 40)\n",
      "(32, 35)\n",
      "(32, 32)\n",
      "(32, 46)\n",
      "(32, 54)\n",
      "(32, 60)\n",
      "(32, 53)\n",
      "(32, 57)\n",
      "(32, 51)\n",
      "(32, 56)\n",
      "(32, 51)\n",
      "(32, 46)\n",
      "(32, 55)\n",
      "(32, 43)\n",
      "(32, 41)\n",
      "(32, 50)\n",
      "(32, 52)\n",
      "(32, 54)\n",
      "(32, 55)\n",
      "(32, 50)\n",
      "(32, 68)\n",
      "(32, 54)\n",
      "(32, 51)\n",
      "(32, 48)\n",
      "(32, 50)\n",
      "(32, 54)\n",
      "(32, 40)\n",
      "(32, 50)\n",
      "(32, 47)\n",
      "(32, 46)\n",
      "(32, 60)\n",
      "(32, 60)\n",
      "(32, 47)\n",
      "(32, 47)\n",
      "(32, 49)\n",
      "(32, 51)\n",
      "(32, 48)\n",
      "(32, 61)\n",
      "(32, 50)\n",
      "(32, 40)\n",
      "(32, 47)\n",
      "(32, 45)\n",
      "(32, 42)\n",
      "(32, 44)\n",
      "(32, 63)\n",
      "(32, 39)\n",
      "(32, 47)\n",
      "(32, 53)\n",
      "(32, 52)\n",
      "(32, 48)\n",
      "(32, 46)\n",
      "(32, 53)\n",
      "(32, 53)\n",
      "(32, 50)\n",
      "(32, 52)\n",
      "(32, 52)\n",
      "(32, 40)\n",
      "(32, 57)\n",
      "(32, 52)\n",
      "(32, 33)\n",
      "(32, 37)\n",
      "(32, 40)\n",
      "(32, 35)\n",
      "(32, 38)\n",
      "(32, 55)\n",
      "(32, 46)\n",
      "(32, 54)\n",
      "(32, 58)\n",
      "(32, 65)\n",
      "(32, 56)\n",
      "(32, 48)\n",
      "(32, 42)\n",
      "(32, 54)\n",
      "(32, 73)\n",
      "(32, 52)\n",
      "(32, 50)\n",
      "(32, 60)\n",
      "(32, 47)\n",
      "(32, 52)\n",
      "(32, 57)\n",
      "(32, 59)\n",
      "(32, 52)\n",
      "(32, 56)\n",
      "(32, 45)\n",
      "(32, 45)\n",
      "(32, 36)\n",
      "(32, 41)\n",
      "(32, 42)\n",
      "(32, 46)\n",
      "(32, 44)\n",
      "(32, 50)\n",
      "(32, 37)\n",
      "(32, 33)\n",
      "(32, 36)\n",
      "(32, 47)\n",
      "(32, 43)\n",
      "(32, 52)\n",
      "(32, 44)\n",
      "(32, 48)\n",
      "(32, 46)\n",
      "(32, 50)\n",
      "(32, 58)\n",
      "(32, 42)\n",
      "(32, 44)\n",
      "(32, 36)\n",
      "(32, 54)\n",
      "(32, 37)\n",
      "(32, 38)\n",
      "(32, 32)\n",
      "(32, 31)\n",
      "(32, 33)\n",
      "(32, 45)\n",
      "(32, 56)\n",
      "(32, 58)\n",
      "(32, 41)\n",
      "(32, 40)\n",
      "(32, 40)\n",
      "(32, 41)\n",
      "(32, 40)\n",
      "(32, 39)\n",
      "(32, 55)\n",
      "(32, 52)\n",
      "(32, 37)\n",
      "(32, 38)\n",
      "(32, 34)\n",
      "(32, 45)\n",
      "(32, 51)\n",
      "(32, 47)\n",
      "(32, 50)\n",
      "(32, 55)\n",
      "(32, 60)\n",
      "(32, 51)\n",
      "(32, 53)\n",
      "(32, 58)\n",
      "(32, 53)\n",
      "(32, 53)\n",
      "(32, 55)\n",
      "(32, 44)\n",
      "(32, 50)\n",
      "(32, 45)\n",
      "(32, 51)\n",
      "(32, 37)\n",
      "(32, 41)\n",
      "(32, 49)\n",
      "(32, 50)\n",
      "(32, 53)\n",
      "(32, 46)\n",
      "(32, 41)\n",
      "(32, 53)\n",
      "(32, 52)\n",
      "(32, 51)\n",
      "(32, 54)\n",
      "(32, 53)\n",
      "(32, 65)\n",
      "(32, 57)\n",
      "(32, 60)\n",
      "(32, 54)\n",
      "(32, 55)\n",
      "(32, 64)\n",
      "(32, 55)\n",
      "(32, 65)\n",
      "(32, 54)\n",
      "(32, 52)\n",
      "(32, 55)\n",
      "(32, 58)\n",
      "(32, 64)\n",
      "(32, 52)\n",
      "(32, 53)\n",
      "(32, 61)\n",
      "(32, 47)\n",
      "(32, 62)\n",
      "(32, 70)\n",
      "(32, 53)\n",
      "(32, 63)\n",
      "(32, 49)\n",
      "(32, 45)\n",
      "(32, 63)\n",
      "(32, 81)\n",
      "(32, 96)\n",
      "(32, 66)\n",
      "(32, 68)\n",
      "(32, 75)\n",
      "(32, 58)\n",
      "(32, 69)\n",
      "(32, 77)\n",
      "(32, 70)\n",
      "(32, 75)\n",
      "(32, 52)\n",
      "(32, 68)\n",
      "(32, 62)\n",
      "(32, 68)\n",
      "(32, 63)\n",
      "(32, 65)\n",
      "(32, 53)\n",
      "(32, 61)\n",
      "(32, 59)\n",
      "(32, 62)\n",
      "(32, 72)\n",
      "(32, 52)\n",
      "(32, 62)\n",
      "(32, 50)\n",
      "(32, 57)\n",
      "(32, 50)\n",
      "(32, 65)\n",
      "(32, 63)\n",
      "(32, 55)\n",
      "(32, 45)\n",
      "(32, 46)\n",
      "(32, 44)\n",
      "(32, 53)\n",
      "(32, 45)\n",
      "(32, 35)\n",
      "(32, 51)\n",
      "(32, 53)\n",
      "(32, 61)\n",
      "(32, 48)\n",
      "(32, 59)\n",
      "(32, 35)\n",
      "(32, 47)\n",
      "(32, 41)\n",
      "(32, 44)\n",
      "(32, 63)\n",
      "(32, 50)\n",
      "(32, 58)\n",
      "(32, 56)\n",
      "(32, 46)\n",
      "(32, 37)\n",
      "(32, 38)\n",
      "(32, 37)\n",
      "(32, 41)\n",
      "(32, 50)\n",
      "(32, 45)\n",
      "(32, 62)\n",
      "(32, 52)\n",
      "(32, 58)\n",
      "(32, 53)\n",
      "(32, 53)\n",
      "(32, 57)\n",
      "(32, 61)\n",
      "(32, 51)\n",
      "(32, 38)\n",
      "(32, 38)\n",
      "(32, 49)\n",
      "(32, 44)\n",
      "(32, 50)\n",
      "(32, 51)\n",
      "(32, 49)\n",
      "(32, 59)\n",
      "(32, 49)\n",
      "(32, 39)\n",
      "(32, 31)\n",
      "(32, 30)\n",
      "(32, 35)\n",
      "(32, 32)\n",
      "(32, 31)\n",
      "(32, 37)\n",
      "(32, 43)\n",
      "(32, 34)\n",
      "(32, 40)\n",
      "(32, 46)\n",
      "(32, 36)\n",
      "(32, 35)\n",
      "(32, 39)\n",
      "(32, 42)\n",
      "(32, 64)\n",
      "(32, 40)\n",
      "(32, 33)\n",
      "(32, 50)\n",
      "(32, 37)\n",
      "(32, 45)\n",
      "(32, 34)\n",
      "(32, 50)\n",
      "(32, 42)\n",
      "(32, 38)\n",
      "(32, 39)\n",
      "(32, 43)\n",
      "(32, 46)\n",
      "(32, 43)\n",
      "(32, 65)\n",
      "(32, 54)\n",
      "(32, 54)\n",
      "(32, 34)\n",
      "(32, 42)\n",
      "(32, 38)\n",
      "(32, 43)\n",
      "(32, 43)\n",
      "(32, 55)\n",
      "(32, 42)\n",
      "(32, 42)\n",
      "(32, 57)\n",
      "(32, 38)\n",
      "(32, 48)\n",
      "(32, 39)\n",
      "(32, 55)\n",
      "(32, 54)\n",
      "(32, 48)\n",
      "(32, 48)\n",
      "(32, 38)\n",
      "(32, 44)\n",
      "(32, 42)\n",
      "(32, 38)\n",
      "(32, 41)\n",
      "(32, 61)\n",
      "(32, 46)\n",
      "(32, 54)\n",
      "(32, 47)\n",
      "(32, 43)\n",
      "(32, 45)\n",
      "(32, 35)\n",
      "(32, 47)\n",
      "(32, 40)\n",
      "(32, 39)\n",
      "(32, 42)\n",
      "(32, 41)\n",
      "(32, 46)\n",
      "(32, 46)\n",
      "(32, 43)\n",
      "(32, 44)\n",
      "(32, 43)\n",
      "(32, 42)\n",
      "(32, 51)\n",
      "(32, 45)\n",
      "(32, 45)\n",
      "(32, 44)\n",
      "(32, 38)\n",
      "(32, 43)\n",
      "(32, 36)\n",
      "(32, 45)\n",
      "(32, 60)\n",
      "(32, 46)\n",
      "(32, 39)\n",
      "(32, 36)\n",
      "(32, 34)\n",
      "(32, 39)\n",
      "(32, 32)\n",
      "(32, 38)\n",
      "(32, 35)\n",
      "(32, 39)\n",
      "(32, 32)\n",
      "(32, 36)\n",
      "(32, 37)\n",
      "(32, 39)\n",
      "(32, 36)\n",
      "(32, 38)\n",
      "(32, 36)\n",
      "(32, 36)\n",
      "(32, 45)\n",
      "(32, 34)\n",
      "(32, 42)\n",
      "(32, 46)\n",
      "(32, 43)\n",
      "(32, 49)\n",
      "(32, 57)\n",
      "(32, 51)\n",
      "(32, 51)\n",
      "(32, 48)\n",
      "(32, 54)\n",
      "(32, 46)\n",
      "(32, 54)\n",
      "(32, 52)\n",
      "(32, 45)\n",
      "(32, 44)\n",
      "(32, 50)\n",
      "(32, 46)\n",
      "(32, 48)\n",
      "(32, 50)\n",
      "(32, 52)\n",
      "(32, 56)\n",
      "(32, 49)\n",
      "(32, 35)\n",
      "(32, 40)\n",
      "(32, 37)\n",
      "(32, 45)\n",
      "(32, 38)\n",
      "(32, 42)\n",
      "(32, 44)\n",
      "(32, 38)\n",
      "(32, 43)\n",
      "(32, 48)\n",
      "(32, 44)\n",
      "(32, 35)\n",
      "(32, 42)\n",
      "(32, 43)\n",
      "(32, 29)\n",
      "(32, 31)\n",
      "(32, 38)\n",
      "(32, 35)\n",
      "(32, 36)\n",
      "(32, 32)\n",
      "(32, 35)\n",
      "(32, 68)\n",
      "(32, 34)\n",
      "(32, 42)\n",
      "(32, 38)\n",
      "(32, 45)\n",
      "(32, 45)\n",
      "(32, 48)\n",
      "(32, 48)\n",
      "(32, 36)\n",
      "(32, 32)\n",
      "(32, 32)\n",
      "(32, 38)\n",
      "(32, 36)\n",
      "(32, 35)\n",
      "(32, 42)\n",
      "(32, 41)\n",
      "(32, 46)\n",
      "(32, 43)\n",
      "(32, 35)\n",
      "(32, 39)\n",
      "(32, 37)\n",
      "(32, 36)\n",
      "(32, 46)\n",
      "(32, 43)\n",
      "(32, 35)\n",
      "(32, 37)\n",
      "(32, 39)\n",
      "(32, 32)\n",
      "(32, 41)\n",
      "(32, 38)\n",
      "(32, 59)\n",
      "(32, 49)\n",
      "(32, 50)\n",
      "(32, 51)\n",
      "(32, 44)\n",
      "(32, 43)\n",
      "(32, 43)\n",
      "(32, 53)\n",
      "(32, 48)\n",
      "(32, 53)\n",
      "(32, 54)\n",
      "(32, 53)\n",
      "(32, 59)\n",
      "(32, 58)\n",
      "(32, 42)\n",
      "(32, 61)\n",
      "(32, 44)\n",
      "(32, 55)\n",
      "(32, 49)\n",
      "(32, 49)\n",
      "(32, 65)\n",
      "(32, 51)\n",
      "(32, 50)\n",
      "(32, 46)\n",
      "(32, 47)\n",
      "(32, 44)\n",
      "(32, 46)\n",
      "(32, 46)\n",
      "(32, 47)\n",
      "(32, 39)\n",
      "(32, 66)\n",
      "(32, 47)\n",
      "(32, 63)\n",
      "(32, 39)\n",
      "(32, 49)\n",
      "(32, 55)\n",
      "(32, 49)\n",
      "(32, 49)\n",
      "(32, 45)\n",
      "(32, 37)\n",
      "(32, 47)\n",
      "(32, 39)\n",
      "(32, 37)\n",
      "(32, 42)\n",
      "(32, 28)\n",
      "(32, 35)\n",
      "(32, 43)\n",
      "(32, 30)\n",
      "(32, 29)\n",
      "(32, 29)\n",
      "(32, 33)\n",
      "(32, 36)\n",
      "(32, 36)\n",
      "(32, 30)\n",
      "(32, 55)\n",
      "(32, 50)\n",
      "(32, 48)\n",
      "(32, 39)\n",
      "(32, 61)\n",
      "(32, 56)\n",
      "(32, 49)\n",
      "(32, 47)\n",
      "(32, 53)\n",
      "(32, 49)\n",
      "(32, 54)\n",
      "(32, 71)\n",
      "(32, 58)\n",
      "(32, 73)\n",
      "(32, 47)\n",
      "(32, 47)\n",
      "(32, 35)\n",
      "(32, 33)\n",
      "(32, 36)\n",
      "(32, 47)\n",
      "(32, 33)\n",
      "(32, 70)\n",
      "(32, 63)\n",
      "(32, 81)\n",
      "(32, 75)\n",
      "(32, 48)\n",
      "(32, 50)\n",
      "(32, 64)\n",
      "(32, 56)\n",
      "(32, 56)\n",
      "(32, 45)\n",
      "(32, 59)\n",
      "(32, 53)\n",
      "(32, 54)\n",
      "(32, 55)\n",
      "(32, 58)\n",
      "(32, 61)\n",
      "(32, 53)\n",
      "(32, 48)\n",
      "(32, 46)\n",
      "(32, 52)\n",
      "(32, 54)\n",
      "(32, 56)\n",
      "(32, 53)\n",
      "(32, 61)\n",
      "(32, 52)\n",
      "(32, 50)\n",
      "(32, 63)\n",
      "(32, 58)\n",
      "(32, 48)\n",
      "(32, 46)\n",
      "(32, 77)\n",
      "(32, 44)\n",
      "(32, 58)\n",
      "(32, 49)\n",
      "(32, 57)\n",
      "(32, 48)\n",
      "(32, 66)\n",
      "(32, 58)\n",
      "(32, 61)\n",
      "(32, 57)\n",
      "(32, 68)\n",
      "(32, 49)\n",
      "(32, 66)\n",
      "(32, 61)\n",
      "(32, 53)\n",
      "(32, 61)\n",
      "(32, 59)\n",
      "(32, 45)\n",
      "(32, 51)\n",
      "(32, 41)\n",
      "(32, 40)\n",
      "(32, 39)\n",
      "(32, 51)\n",
      "(32, 54)\n",
      "(32, 46)\n",
      "(32, 48)\n",
      "(32, 44)\n",
      "(32, 43)\n",
      "(32, 42)\n",
      "(32, 49)\n",
      "(32, 39)\n",
      "(32, 45)\n",
      "(32, 42)\n",
      "(32, 40)\n",
      "(32, 47)\n",
      "(32, 45)\n",
      "(32, 41)\n",
      "(32, 46)\n",
      "(32, 54)\n",
      "(32, 39)\n",
      "(32, 52)\n",
      "(32, 51)\n",
      "(32, 57)\n",
      "(32, 53)\n",
      "(32, 52)\n",
      "(32, 44)\n",
      "(32, 46)\n",
      "(32, 49)\n",
      "(32, 51)\n",
      "(32, 56)\n",
      "(32, 49)\n",
      "(32, 47)\n",
      "(32, 54)\n",
      "(32, 51)\n",
      "(32, 47)\n",
      "(32, 56)\n",
      "(32, 61)\n",
      "(32, 55)\n",
      "(32, 47)\n",
      "(32, 44)\n",
      "(32, 43)\n",
      "(32, 45)\n",
      "(32, 80)\n",
      "(32, 67)\n",
      "(32, 60)\n",
      "(32, 56)\n",
      "(32, 46)\n",
      "(32, 87)\n",
      "(32, 60)\n",
      "(32, 57)\n",
      "(32, 51)\n",
      "(32, 71)\n",
      "(32, 56)\n",
      "(32, 68)\n",
      "(32, 50)\n",
      "(32, 55)\n",
      "(32, 53)\n",
      "(32, 53)\n",
      "(32, 70)\n",
      "(32, 59)\n",
      "(32, 53)\n",
      "(32, 41)\n",
      "(32, 54)\n",
      "(32, 65)\n",
      "(32, 55)\n",
      "(32, 60)\n",
      "(32, 54)\n",
      "(32, 64)\n",
      "(32, 54)\n",
      "(32, 49)\n",
      "(32, 57)\n",
      "(32, 76)\n",
      "(32, 56)\n",
      "(32, 48)\n",
      "(32, 52)\n",
      "(32, 77)\n",
      "(32, 92)\n",
      "(32, 61)\n",
      "(32, 64)\n",
      "(32, 52)\n",
      "(32, 46)\n",
      "(32, 55)\n",
      "(32, 56)\n",
      "(32, 57)\n",
      "(32, 46)\n",
      "(32, 45)\n",
      "(32, 61)\n",
      "(32, 52)\n",
      "(32, 61)\n",
      "(32, 64)\n",
      "(32, 68)\n",
      "(32, 63)\n",
      "(32, 90)\n",
      "(32, 61)\n",
      "(32, 65)\n",
      "(32, 85)\n",
      "(32, 50)\n",
      "(32, 57)\n",
      "(32, 40)\n",
      "(32, 47)\n",
      "(32, 44)\n",
      "(32, 38)\n",
      "(32, 40)\n",
      "(32, 35)\n",
      "(32, 41)\n",
      "(32, 41)\n",
      "(32, 45)\n",
      "(32, 60)\n",
      "(32, 55)\n",
      "(32, 55)\n",
      "(32, 57)\n",
      "(32, 78)\n",
      "(32, 58)\n",
      "(32, 47)\n",
      "(32, 49)\n",
      "(32, 62)\n",
      "(32, 68)\n",
      "(32, 50)\n",
      "(32, 49)\n",
      "(32, 46)\n",
      "(32, 63)\n",
      "(32, 54)\n"
     ]
    }
   ],
   "source": [
    "for batch in dataset:\n",
    "    print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|> 제가 지금 해운대에서 여행하면서 공연을 보려고 예매했는데요 사이트에서 안내된 거랑 다르게 추가로 결제가 더 된 것 같아서요 고객님 영화의 전당 사이트에서 예매하셨을까요 <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9873, 34791, 19352, 11649, 13023, 9402, 8084]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode('네 예약하려면 어떻게 해야 하나요'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1074\n"
     ]
    }
   ],
   "source": [
    "steps = len(df) // batch_size + 1\n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 54])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe338c0e4b54439a1cdbbbd7c21eee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1074 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer \"transformer\" (type TFGPT2MainLayer).\n\ninput_ids must be smaller than the embedding layer's input dimension (got 51200 >= 51200)\nCondition x < y did not hold.\nFirst 3 elements of x:\n[51200  9665  6919]\nFirst 1 elements of y:\n[51200]\n\nCall arguments received by layer \"transformer\" (type TFGPT2MainLayer):\n  • input_ids=tf.Tensor(shape=(32, 36), dtype=int32)\n  • past_key_values=None\n  • attention_mask=None\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • use_cache=True\n  • output_attentions=False\n  • output_hidden_states=False\n  • return_dict=True\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[169], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mnotebook\u001b[39m.\u001b[39mtqdm(dataset, total\u001b[39m=\u001b[39msteps):\n\u001b[0;32m      7\u001b[0m     \u001b[39m# tf.GradientTape() : 실행된 모든 연산은 테이프에 기록\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m----> 9\u001b[0m         result \u001b[39m=\u001b[39m model(batch, labels\u001b[39m=\u001b[39;49mbatch)\n\u001b[0;32m     10\u001b[0m         loss \u001b[39m=\u001b[39m result[\u001b[39m0\u001b[39m]\n\u001b[0;32m     11\u001b[0m         batch_loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_mean(loss)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\DL\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\DL\\lib\\site-packages\\transformers\\modeling_tf_utils.py:432\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\n\u001b[0;32m    431\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m input_processing(func, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_args_and_kwargs)\n\u001b[1;32m--> 432\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munpacked_inputs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\DL\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py:899\u001b[0m, in \u001b[0;36mTFGPT2LMHeadModel.call\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[39m@unpack_inputs\u001b[39m\n\u001b[0;32m    851\u001b[0m \u001b[39m@add_start_docstrings_to_model_forward\u001b[39m(GPT2_INPUTS_DOCSTRING)\n\u001b[0;32m    852\u001b[0m \u001b[39m@add_code_sample_docstrings\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    873\u001b[0m     training: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    874\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[TFCausalLMOutputWithCrossAttentions, Tuple[tf\u001b[39m.\u001b[39mTensor]]:\n\u001b[0;32m    875\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[39m    encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    896\u001b[0m \u001b[39m        config.vocab_size - 1]`.\u001b[39;00m\n\u001b[0;32m    897\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 899\u001b[0m     transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m    900\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m    901\u001b[0m         past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    902\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    903\u001b[0m         token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m    904\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m    905\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    906\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    907\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    908\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m    909\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    910\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    911\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    912\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    913\u001b[0m         training\u001b[39m=\u001b[39;49mtraining,\n\u001b[0;32m    914\u001b[0m     )\n\u001b[0;32m    915\u001b[0m     hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    916\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mwte(hidden_states, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\DL\\lib\\site-packages\\transformers\\modeling_tf_utils.py:432\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\n\u001b[0;32m    431\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m input_processing(func, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_args_and_kwargs)\n\u001b[1;32m--> 432\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munpacked_inputs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\DL\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py:442\u001b[0m, in \u001b[0;36mTFGPT2MainLayer.call\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    437\u001b[0m position_ids \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreshape(position_ids, [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, shape_list(position_ids)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]])\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    440\u001b[0m     \u001b[39m# Note: tf.gather, on which the embedding layer is based, won't check positive out of bound\u001b[39;00m\n\u001b[0;32m    441\u001b[0m     \u001b[39m# indices on GPU, returning zeros instead. This is a dangerous silent behavior.\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m     tf\u001b[39m.\u001b[39;49mdebugging\u001b[39m.\u001b[39;49massert_less(\n\u001b[0;32m    443\u001b[0m         input_ids,\n\u001b[0;32m    444\u001b[0m         tf\u001b[39m.\u001b[39;49mcast(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mvocab_size, dtype\u001b[39m=\u001b[39;49minput_ids\u001b[39m.\u001b[39;49mdtype),\n\u001b[0;32m    445\u001b[0m         message\u001b[39m=\u001b[39;49m(\n\u001b[0;32m    446\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39minput_ids must be smaller than the embedding layer\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39ms input dimension (got\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m    447\u001b[0m             \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m{\u001b[39;49;00mtf\u001b[39m.\u001b[39;49mmath\u001b[39m.\u001b[39;49mreduce_max(input_ids)\u001b[39m}\u001b[39;49;00m\u001b[39m >= \u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mvocab_size\u001b[39m}\u001b[39;49;00m\u001b[39m)\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m    448\u001b[0m         ),\n\u001b[0;32m    449\u001b[0m     )\n\u001b[0;32m    450\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwte(input_ids, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    452\u001b[0m position_embeds \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mgather(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwpe, position_ids)\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"transformer\" (type TFGPT2MainLayer).\n\ninput_ids must be smaller than the embedding layer's input dimension (got 51200 >= 51200)\nCondition x < y did not hold.\nFirst 3 elements of x:\n[51200  9665  6919]\nFirst 1 elements of y:\n[51200]\n\nCall arguments received by layer \"transformer\" (type TFGPT2MainLayer):\n  • input_ids=tf.Tensor(shape=(32, 36), dtype=int32)\n  • past_key_values=None\n  • attention_mask=None\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • use_cache=True\n  • output_attentions=False\n  • output_hidden_states=False\n  • return_dict=True\n  • training=False"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in tqdm.notebook.tqdm(dataset, total=steps):\n",
    "        # tf.GradientTape() : 실행된 모든 연산은 테이프에 기록\n",
    "        with tf.GradientTape() as tape:\n",
    "            result = model(batch, labels=batch)\n",
    "            loss = result[0]\n",
    "            batch_loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        grads = tape.gradient(batch_loss, model.trainable_variables)\n",
    "        adam.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        epoch_loss += batch_loss / steps\n",
    "\n",
    "    print('[Epoch: {:>4}] cost = {:>9}'.format(epoch + 1, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
