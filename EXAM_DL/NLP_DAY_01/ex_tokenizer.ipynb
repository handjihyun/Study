{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자연어 처리 (NLP) - 전처리\n",
    "- 1단계 : 형태소(즉, 의미를 가지는 가장 작은 단위)로 분리 ==> 토큰화\n",
    "    * 토큰(Token) : 분리된 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 로딩\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tokenizer object\n",
    "NUM_WORDS = 1000    # word dictionary count of dataset\n",
    "OOV = '<UNK>'       # character that isn't in word dictionary\n",
    "\n",
    "tokenizer = Tokenizer(num_words = NUM_WORDS, oov_token = OOV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['Today is a sunny day',\n",
    "'Today is a rainy day',\n",
    "'Is it sunny today?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make word dictionary based on \"sentences\" data\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.preprocessing.text.Tokenizer"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 1,\n",
       " 'today': 2,\n",
       " 'is': 3,\n",
       " 'a': 4,\n",
       " 'sunny': 5,\n",
       " 'day': 6,\n",
       " 'rainy': 7,\n",
       " 'it': 8}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('today', 3),\n",
       "             ('is', 3),\n",
       "             ('a', 2),\n",
       "             ('sunny', 2),\n",
       "             ('day', 2),\n",
       "             ('rainy', 1),\n",
       "             ('it', 1)])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 4, 5, 6], [2, 3, 4, 7, 6], [3, 8, 5, 2]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert sentence => number\n",
    "# sentences = ['Today is a sunny day',\n",
    "#               'Today is a rainy day',\n",
    "#               'Is it sunny today?']\n",
    "tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 4, 5, 6], [1, 8, 1, 7, 1]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert new sentence => number\n",
    "data = ['Today is a sunny day.', 'Will it be rainy tomorrow?']\n",
    "\n",
    "ret = tokenizer.texts_to_sequences(data)\n",
    "ret\n",
    "# if word is not in dictionary, don't print number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today is a sunny day', '<UNK> it <UNK> rainy <UNK>']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert number sentence => text sentence\n",
    "tokenizer.sequences_to_texts(ret)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Data Cleaning\n",
    "- 수시로 진행될 수 있음\n",
    "- 토큰화 전에 실행 또는 후에 실행도 가능\n",
    "- 정제란?\n",
    "    * 문장에서 의미가 없는 단어 즉, 조사, 관사 ==> stopword 불용어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['Today is a sunny day',\n",
    "              'Today is a rainy day',\n",
    "              'Is it sunny today?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = ['a', 'the', 'this', 'that', 'she', 'he', 'i', 'me', 'below', 'above']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split present text => whitespace\n",
    "my_words = []\n",
    "\n",
    "for st in sentences:\n",
    "    ret = st.split()\n",
    "    for _ in ret:\n",
    "        if _ not in stopword:\n",
    "            my_words.append(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5, 6]\n",
    "values = []\n",
    "\n",
    "def check(data):\n",
    "    if data % 2:\n",
    "        return data\n",
    "\n",
    "list(filter(check, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda d : d % 2, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0, 1, 0]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda d : d % 2, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 구두\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
